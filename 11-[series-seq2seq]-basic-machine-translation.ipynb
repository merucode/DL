{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merucode/DL/blob/01-colab-study_must_have_pytorch/11-%5Bseries-seq2seq%5D-basic-machine-translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 0. Version check and Install Dependency"
      ],
      "metadata": {
        "id": "LxkMBWDW9T3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 0-1. Version Check"
      ],
      "metadata": {
        "id": "0Fg2WcTOqf5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "print(f\"Python version:{sys.version}\")                  # python\n",
        "print(\"Torch version:{}\".format(torch.__version__))     # torch\n",
        "print(\"cuda version: {}\".format(torch.version.cuda))    # cuda\n",
        "print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))    # cudnn"
      ],
      "metadata": {
        "id": "pvZ7Lm3Apv1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 0-2. Install Dependency"
      ],
      "metadata": {
        "id": "cNpk3WdHys1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.manythings.org/anki/kor-eng.zip\n",
        "!unzip kor-eng.zip"
      ],
      "metadata": {
        "id": "1vlduSlSwM9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1. Check Data"
      ],
      "metadata": {
        "id": "TquybxQaqfE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1-1. Load data"
      ],
      "metadata": {
        "id": "OiEgwO4nzBQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "l = []\n",
        "\n",
        "with open(\"/content/kor.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "   lines = f.read().split(\"\\n\")\n",
        "   new_lines = ''\n",
        "\n",
        "   for line in lines:\n",
        "       line = line.split(\"\\t\")\n",
        "       if len(line) > 2:\n",
        "           line = line[0] + '\\t' + line[1] + '\\n'\n",
        "           new_lines = new_lines + line\n",
        "\n",
        "with open(\"/content/kor_new.txt\", 'w', encoding=\"utf-8\") as f:\n",
        "    f.write(new_lines[:-2]) # 마지막 wn 제거\n",
        "\n",
        "# 한글 텍스트 파일을 읽기 위해 utf-8 인코딩으로 읽어옴\n",
        "with open(\"/content/kor_new.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "   lines = f.read().split(\"\\n\")\n",
        "\n",
        "   for line in lines:\n",
        "       # 특수 문자를 지우고 모든 글자를 소문자로 변경\n",
        "       txt = \"\".join(v for v in line if v not in string.punctuation).lower()\n",
        "       l.append(txt)"
      ],
      "metadata": {
        "id": "O_1DAWvLxNST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1-2. Check data type"
      ],
      "metadata": {
        "id": "yLz34AIjzFI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(l[:5])"
      ],
      "metadata": {
        "id": "BIwItmZr48ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 2. Dataset"
      ],
      "metadata": {
        "id": "dHoQK0Vw5UWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2-1. BOW"
      ],
      "metadata": {
        "id": "B4jMESbH5T7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "def get_BOW(corpus):  # 문장들로부터 BOW를 만드는 함수\n",
        "   BOW = {\"<SOS>\":0, \"<EOS>\":1}  # ❶ <SOS> 토큰과 <EOS> 토큰을 추가\n",
        "\n",
        "   # ❷ 문장 내 단어들을 이용해 BOW를 생성\n",
        "   for line in corpus:\n",
        "       for word in line.split():\n",
        "           if word not in BOW.keys():\n",
        "               BOW[word] = len(BOW.keys())\n",
        "\n",
        "   return BOW"
      ],
      "metadata": {
        "id": "sA6bIF381UTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2-2. Dataset"
      ],
      "metadata": {
        "id": "PxW5Kc551RCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Eng2Kor(Dataset):  # 학습에 이용할 데이터셋\n",
        "   def __init__(\n",
        "       self,\n",
        "       pth2txt=\\\n",
        "       \"/content/kor_new.txt\"):\n",
        "       self.eng_corpus = []  # 영어 문장이 들어가는 변수\n",
        "       self.kor_corpus = []  # 한글 문장이 들어가는 변수\n",
        "\n",
        "       # ➊ 텍스트 파일을 읽어서 영어 문장과 한글 문장을 저장\n",
        "       with open(pth2txt, 'r', encoding=\"utf-8\") as f:\n",
        "           lines = f.read().split(\"\\n\")\n",
        "           for line in lines:\n",
        "               # 특수 문자와 대문자 제거\n",
        "               txt = \"\".join(\n",
        "                   v for v in line if v not in string.punctuation\n",
        "                   ).lower()\n",
        "               engtxt = txt.split(\"\\t\")[0]\n",
        "               kortxt = txt.split(\"\\t\")[1]\n",
        "\n",
        "               # 길이가 10 이하인 문장만을 사용\n",
        "               if len(engtxt.split()) <= 10 and len(kortxt.split()) <= 10:\n",
        "                   self.eng_corpus.append(engtxt)\n",
        "                   self.kor_corpus.append(kortxt)\n",
        "\n",
        "       self.engBOW = get_BOW(self.eng_corpus)  # 영어 BOW\n",
        "       self.korBOW = get_BOW(self.kor_corpus)  # 한글 BOW\n",
        "   # 문장을 단어별로 분리하고 마지막에 <EOS>를 추가\n",
        "   def gen_seq(self, line):\n",
        "       seq = line.split()\n",
        "       seq.append(\"<EOS>\")\n",
        "\n",
        "       return seq\n",
        "   def __len__(self): # ❶\n",
        "       return len(self.eng_corpus)\n",
        "\n",
        "   def __getitem__(self, i): # ❷\n",
        "       # 문자열로 되어 있는 문장을 숫자 표현으로 변경\n",
        "       data = np.array([\n",
        "            self.engBOW[txt] for txt in self.gen_seq(self.eng_corpus[i])])\n",
        "\n",
        "       label = np.array([\n",
        "            self.korBOW[txt] for txt in self.gen_seq(self.kor_corpus[i])])\n",
        "\n",
        "       return data, label"
      ],
      "metadata": {
        "id": "sie_qD2V1RSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2-3. Dataloader"
      ],
      "metadata": {
        "id": "7js8NuXt6Fjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loader(dataset):  # 데이터셋의 문장을 한문장씩 불러오기 위한 함수\n",
        "   for i in range(len(dataset)):\n",
        "       data, label = dataset[i]\n",
        "\n",
        "       # ❶ 데이터와 정답을 반환\n",
        "       yield torch.tensor(data), torch.tensor(label)"
      ],
      "metadata": {
        "id": "K_gYrRErAGxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 3. Module"
      ],
      "metadata": {
        "id": "UPv6ybM8zJYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3-1. Encoder"
      ],
      "metadata": {
        "id": "QQnNmDIS5iyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "   def __init__(self, input_size, hidden_size):\n",
        "       super(Encoder, self).__init__()\n",
        "\n",
        "       self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "       self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "   def forward(self, x, h):\n",
        "       # ❶ 배치차원과 시계열 차원 추가\n",
        "       x = self.embedding(x).view(1, 1, -1)\n",
        "       output, hidden = self.gru(x, h)\n",
        "       return output, hidden"
      ],
      "metadata": {
        "id": "ccWmGvzCALyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3-2. Decoder"
      ],
      "metadata": {
        "id": "Bn-rSzZVANE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "   def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=11):\n",
        "       super(Decoder, self).__init__()\n",
        "\n",
        "       # 임베딩층 정의\n",
        "       self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "\n",
        "       # 어텐션 가중치를 계산하기 위한 MLP층\n",
        "       self.attention = nn.Linear(hidden_size * 2, max_length)\n",
        "\n",
        "       #특징 추출을 위한 MLP층\n",
        "       self.context = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "       # 과적합을 피하기 위한 드롭아웃 층\n",
        "       self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "       # GRU층\n",
        "       self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "       # 단어 분류를 위한 MLP층\n",
        "       self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "       # 활성화 함수\n",
        "       self.relu = nn.ReLU()\n",
        "       self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "   def forward(self, x, h, encoder_outputs):\n",
        "       # ➊입력을 밀집 표현으로\n",
        "       x = self.embedding(x).view(1, 1, -1)\n",
        "       x = self.dropout(x)\n",
        "\n",
        "       # ➋어텐션 가중치 계산\n",
        "       attn_weights = self.softmax(\n",
        "           self.attention(torch.cat((x[0], h[0]), -1)))\n",
        "\n",
        "       # ➌어텐션 가중치와 인코더의 출력을 내적\n",
        "       attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                encoder_outputs.unsqueeze(0))\n",
        "\n",
        "       # ➍인코더 각 시점의 중요도와 민집표현을 합쳐\n",
        "       # MLP층으로 특징 추출\n",
        "       output = torch.cat((x[0], attn_applied[0]), 1)\n",
        "       output = self.context(output).unsqueeze(0)\n",
        "       output = self.relu(output)\n",
        "\n",
        "       # ➎GRU층으로 입력\n",
        "       output, hidden = self.gru(output, h)\n",
        "\n",
        "       # ➏예측된 단어 출력\n",
        "       output = self.out(output[0])\n",
        "\n",
        "       return output"
      ],
      "metadata": {
        "id": "Zj7eVs2XAPEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 4. Learning"
      ],
      "metadata": {
        "id": "Qa9kBj02zz7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4-1. Setting"
      ],
      "metadata": {
        "id": "cg8ZCYcJ0JIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import tqdm\n",
        "\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "# 학습에 사용할 프로세서 정의\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# 학습에 사용할 데이터셋 정의\n",
        "dataset = Eng2Kor()\n",
        "\n",
        "# 인코더 디코더 정의\n",
        "encoder = Encoder(input_size=len(dataset.engBOW), hidden_size=64).to(device)\n",
        "decoder = Decoder(64, len(dataset.korBOW), dropout_p=0.1).to(device)\n",
        "\n",
        "# 인코더 디코더 학습을 위한 최적화 정의\n",
        "encoder_optimizer = Adam(encoder.parameters(), lr=0.0001)\n",
        "decoder_optimizer = Adam(decoder.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "B8clGgz65sxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4-2. Learning"
      ],
      "metadata": {
        "id": "lH9EvJSJz-AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for epoch in range(200):\n",
        "for epoch in range(25):\n",
        "   iterator = tqdm.tqdm(loader(dataset), total=len(dataset))\n",
        "   total_loss = 0\n",
        "\n",
        "   for data, label in iterator:\n",
        "       data = torch.tensor(data, dtype=torch.long).to(device)\n",
        "       label = torch.tensor(label, dtype=torch.long).to(device)\n",
        "\n",
        "       # 인코더의 초기 은닉 상태\n",
        "       encoder_hidden = torch.zeros(1, 1, 64).to(device)\n",
        "       # 인코더의 모든 시점의 출력을 저장하는 변수\n",
        "       encoder_outputs = torch.zeros(11, 64).to(device)\n",
        "\n",
        "       encoder_optimizer.zero_grad()\n",
        "       decoder_optimizer.zero_grad()\n",
        "\n",
        "       loss = 0\n",
        "\n",
        "       # 인코더 동작\n",
        "       for ei in range(len(data)):\n",
        "           # ➊한 단어씩 인코더에 넣어줌\n",
        "           encoder_output, encoder_hidden = encoder(data[ei], encoder_hidden)\n",
        "           # ❷인코더의 은닉 상태를 저장\n",
        "           encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "       # <SOE> 토큰 관련\n",
        "       decoder_input = torch.tensor([[0]]).to(device)\n",
        "\n",
        "       # ❸인코더의 마지막 은닉 상태를 디코더의 초기 은닉 상태로 저장\n",
        "       decoder_hidden = encoder_hidden\n",
        "\n",
        "       # 디코더 동작\n",
        "       # 티쳐 포싱 사용(50% 확률로 강제 정답 입력)\n",
        "       use_teacher_forcing = True if random.random() < 0.5 else False  # ❶\n",
        "\n",
        "       if use_teacher_forcing:\n",
        "           for di in range(len(label)):\n",
        "               decoder_output = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "               # 직접적으로 정답을 다음 시점의 입력으로 넣어줌\n",
        "               target = torch.tensor(label[di], dtype=torch.long).to(device)\n",
        "               target = torch.unsqueeze(target, dim=0).to(device)\n",
        "               loss += nn.CrossEntropyLoss()(decoder_output, target)\n",
        "               decoder_input = target\n",
        "       else:\n",
        "           for di in range(len(label)):\n",
        "               decoder_output = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "               # ➊ 가장 높은 확률을 갖는 단어의 인덱스가 topi\n",
        "               topv, topi = decoder_output.topk(1)\n",
        "               decoder_input = topi.squeeze().detach()\n",
        "\n",
        "               # 디코더의 예측값을 다음 시점의 입력으로 넣어줌\n",
        "               target = torch.tensor(label[di], dtype=torch.long).to(device)\n",
        "               target = torch.unsqueeze(target, dim=0).to(device)\n",
        "               loss += nn.CrossEntropyLoss()(decoder_output, target)\n",
        "\n",
        "               if decoder_input.item() == 1:  # <EOS> 토큰을 만나면 중지\n",
        "                   break\n",
        "       # 전체 손실 계산\n",
        "       total_loss += loss.item()/len(dataset)\n",
        "       iterator.set_description(f\"epoch:{epoch+1} loss:{total_loss}\")\n",
        "       loss.backward()\n",
        "\n",
        "       encoder_optimizer.step()\n",
        "       decoder_optimizer.step()\n",
        "\n",
        "torch.save(encoder.state_dict(), \"attn_enc.pth\")\n",
        "torch.save(decoder.state_dict(), \"attn_dec.pth\")"
      ],
      "metadata": {
        "id": "krz6kIYz52wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 5. Evaluation"
      ],
      "metadata": {
        "id": "sdk0NFRA085c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5-1. 모델 성능 평가에 필요한 요소 정의"
      ],
      "metadata": {
        "id": "6vHXqqIZEreI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 가중치 불러오기\n",
        "encoder.load_state_dict(torch.load(\"attn_enc.pth\", map_location=device))\n",
        "# 디코더 가중치 불러오기\n",
        "decoder.load_state_dict(torch.load(\"attn_dec.pth\", map_location=device))\n",
        "\n",
        "\n",
        "# ❶불러올 영어 문장을 랜덤하게 지정\n",
        "idx = random.randint(0, len(dataset))\n",
        "# 테스트에 사용할 문장\n",
        "input_sentence = dataset.eng_corpus[idx]\n",
        "# 신경망이 번역한 문장\n",
        "pred_sentence = \"\"\n",
        "\n",
        "data, label = dataset[idx]\n",
        "data = torch.tensor(data, dtype=torch.long).to(device)\n",
        "label = torch.tensor(label, dtype=torch.long).to(device)\n",
        "\n",
        "# ➋인코더의 초기 은닉 상태 정의\n",
        "encoder_hidden = torch.zeros(1, 1, 64).to(device)\n",
        "# 인코더 출력을 담기위한 변수\n",
        "encoder_outputs = torch.zeros(11, 64).to(device)"
      ],
      "metadata": {
        "id": "6t5jawBx8ibI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5-2. Encoder"
      ],
      "metadata": {
        "id": "5Nk8G1LeEr0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ei in range(len(data)):\n",
        "   # ➊한 단어씩 인코더에 넣어줌\n",
        "   encoder_output, encoder_hidden = encoder(\n",
        "       data[ei], encoder_hidden)\n",
        "\n",
        "   # ➋인코더의 출력을 저장\n",
        "   encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "\n",
        "# ➌디코더의 초기 입력\n",
        "# 0은 <SOS>토큰\n",
        "decoder_input = torch.tensor([[0]]).to(device)\n",
        "\n",
        "# ➍인코더의 마지막 은닉 상태를 디코더의 초기 은닉 상태로\n",
        "decoder_hidden = encoder_hidden"
      ],
      "metadata": {
        "id": "FzHVcG2XAwxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5-3. Decoder"
      ],
      "metadata": {
        "id": "wAlI4tJmEsFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for di in range(11):\n",
        "    # ➊가장 높은 확률을 갖는 단어의 요소를 구함\n",
        "   decoder_output = decoder(\n",
        "                       decoder_input, decoder_hidden, encoder_outputs)\n",
        "   topv, topi = decoder_output.topk(1)\n",
        "   decoder_input = topi.squeeze().detach()\n",
        "\n",
        "   # ➋<EOS> 토큰을 만나면 중지\n",
        "   if decoder_input.item() == 1:\n",
        "       break\n",
        "\n",
        "   # ➌가장 높은 단어를 문자열에 추가\n",
        "   pred_sentence += list(dataset.korBOW.keys())[decoder_input] + \" \"\n",
        "\n",
        "print(input_sentence)  # 영어 문장\n",
        "print(pred_sentence)  # 한글 문장"
      ],
      "metadata": {
        "id": "ENeXH1mlEqM4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}