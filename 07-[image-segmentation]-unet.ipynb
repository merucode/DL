{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merucode/DL/blob/01-colab-study_must_have_pytorch/07-%5Bimage-segmentation%5D-unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 0. Version check and Install Dependency"
      ],
      "metadata": {
        "id": "LxkMBWDW9T3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 0-1. Version Check"
      ],
      "metadata": {
        "id": "0Fg2WcTOqf5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "print(f\"Python version:{sys.version}\")                  # python\n",
        "print(\"Torch version:{}\".format(torch.__version__))     # torch\n",
        "print(\"cuda version: {}\".format(torch.version.cuda))    # cuda\n",
        "print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))    # cudnn"
      ],
      "metadata": {
        "id": "pvZ7Lm3Apv1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 0-2. Install Dependency"
      ],
      "metadata": {
        "id": "cNpk3WdHys1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1. Check Data"
      ],
      "metadata": {
        "id": "TquybxQaqfE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1-1. Load data"
      ],
      "metadata": {
        "id": "OiEgwO4nzBQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "from torchvision.transforms import ToTensor, Resize\n",
        "\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "transforms = Compose([\n",
        "   Resize((128, 128)),\n",
        "])\n",
        "\n",
        "training_data = OxfordIIITPet(root=\"./\", split='trainval', target_types='segmentation', download=True)\n",
        "test_data = OxfordIIITPet(root=\"./\", split='test', target_types='segmentation', download=True)\n",
        "\n",
        "print(len(training_data))\n",
        "print(len(test_data))"
      ],
      "metadata": {
        "id": "O_1DAWvLxNST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1-2. Check data type"
      ],
      "metadata": {
        "id": "yLz34AIjzFI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dir(training_data)\n",
        "print(training_data[1])"
      ],
      "metadata": {
        "id": "a3Lelovq5K7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(9): # 샘플(data) 이미지를 9개 출력\n",
        "   plt.subplot(3, 3, i+1)\n",
        "   plt.imshow(transforms(test_data[i][0]))  # transforms를 적용한 경우 이미지, 원본의 경우 'test_data[i][0]' 사용\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BIwItmZr48ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(9): # 샘플(정답) 이미지를 9개 출력\n",
        "   plt.subplot(3, 3, i+1)\n",
        "   plt.imshow(training_data[i][1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dGEvLwjs79jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1-3. Load Data using Path"
      ],
      "metadata": {
        "id": "W3Ws9Ym3FMA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "path_to_annotation = \\\n",
        "    \"./oxford-iiit-pet/annotations/trimaps\"\n",
        "path_to_image = \\\n",
        "    \"./oxford-iiit-pet/images\"\n",
        "\n",
        "annotation = Image.open(path_to_annotation + \"/Abyssinian_1.png\") # 이미지를 불러옴\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"annotation\")\n",
        "plt.imshow(annotation)\n",
        "\n",
        "image = Image.open(path_to_image + \"/Abyssinian_1.jpg\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"image\")\n",
        "plt.imshow(image)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WVa7UNnpFLTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 2. Dataset"
      ],
      "metadata": {
        "id": "dHoQK0Vw5UWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2-1. Custom Dataset"
      ],
      "metadata": {
        "id": "0cU4E3q9Gz7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob  # 이미지를 불러올 때 사용하는 라이브러리\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class Pets(Dataset):\n",
        "   def __init__(self, path_to_img,\n",
        "                path_to_anno,\n",
        "                train=True,\n",
        "                transforms=None,\n",
        "                input_size=(128, 128)):\n",
        "\n",
        "       # ❶ 정답과 입력 이미지를 이름순으로 정렬\n",
        "       self.images = sorted(glob.glob(path_to_img+\"/*.jpg\"))\n",
        "       self.annotations = sorted(glob.glob(path_to_anno + \"/*.png\"))\n",
        "\n",
        "       # ❷ 데이터셋을 학습과 평가용으로 나눔\n",
        "       self.X_train = self.images[:int(0.8 * len(self.images))]\n",
        "       self.X_test = self.images[int(0.8 * len(self.images)):]\n",
        "       self.Y_train = self.annotations[:int(0.8 * len(self.annotations))]\n",
        "       self.Y_test = self.annotations[int(0.8 * len(self.annotations)):]\n",
        "\n",
        "       self.train = train  # 학습용 데이터 평가용 데이터 결정 여부\n",
        "       self.transforms = transforms  # 사용할 데이터 증강\n",
        "       self.input_size = input_size  # 입력 이미지 크기\n",
        "\n",
        "   def __len__(self):  # 데이터 개수를 나타냄\n",
        "       if self.train:\n",
        "           return len(self.X_train)  # 학습용 데이터셋 길이\n",
        "       else:\n",
        "           return len(self.X_test)   # 평가용 데이터셋 길이\n",
        "\n",
        "   # 학습 편이를 위해서 다중분류를 이중분류 문제로 변경\n",
        "   def preprocess_mask(self, mask):  # ➋ 정답을 변환해주는 함수\n",
        "       mask = mask.resize(self.input_size)\n",
        "       mask = np.array(mask).astype(np.float32)\n",
        "       mask[mask != 2.0] = 1.0\n",
        "       mask[mask == 2.0] = 0.0\n",
        "       mask = torch.tensor(mask)\n",
        "       return mask\n",
        "\n",
        "   def __getitem__(self, i):  # i번째 데이터와 정답을 반환\n",
        "       if self.train:  # 학습용 데이터\n",
        "           X_train = Image.open(self.X_train[i])\n",
        "           X_train = self.transforms(X_train)\n",
        "           Y_train = Image.open(self.Y_train[i])\n",
        "           Y_train = self.preprocess_mask(Y_train)\n",
        "           return X_train, Y_train\n",
        "\n",
        "       else:  # 평가용 데이터\n",
        "           X_test = Image.open(self.X_test[i])\n",
        "           X_test = self.transforms(X_test)\n",
        "           Y_test = Image.open(self.Y_test[i])\n",
        "           Y_test = self.preprocess_mask(Y_test)\n",
        "\n",
        "           return X_test, Y_test"
      ],
      "metadata": {
        "id": "B4jMESbH5T7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2-2. DataLoader"
      ],
      "metadata": {
        "id": "LJ7wO4C2HDmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Compose\n",
        "from torchvision.transforms import ToTensor, Resize, ToPILImage\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "# 데이터 전처리 정의\n",
        "transform = Compose([\n",
        "   Resize((128, 128)),\n",
        "   ToTensor()\n",
        "])\n",
        "\n",
        "# 학습 데이터\n",
        "train_set = Pets(path_to_img=path_to_image,\n",
        "                 path_to_anno=path_to_annotation,\n",
        "                 transforms=transform)\n",
        "\n",
        "# 평가용 데이터\n",
        "test_set = Pets(path_to_img=path_to_image,\n",
        "                path_to_anno=path_to_annotation,\n",
        "                transforms=transform,\n",
        "                train=False)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_set)"
      ],
      "metadata": {
        "id": "u5VAGdaKG0bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2-3. 채널 수가 맞지 않는 이미지 확인"
      ],
      "metadata": {
        "id": "o9f5qXc7QDD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trans = Compose([ToPILImage()])\n",
        "\n",
        "not_match_img_idx = []\n",
        "\n",
        "print('train')\n",
        "for i in range(0,len(train_set)):\n",
        "  if train_set[i][0].shape != torch.Size([3, 128, 128]):\n",
        "    print(train_set[i][0].shape, i)\n",
        "    not_match_img_idx.append(i)\n",
        "    #plt.imshow(trans(train_set[i][0]))\n",
        "    #plt.show()\n",
        "\n",
        "print('test')\n",
        "for i in range(0,len(test_set)):\n",
        "  if test_set[i][0].shape != torch.Size([3, 128, 128]):\n",
        "    print(test_set[i][0].shape, len(train_set) + i)\n",
        "    not_match_img_idx.append(len(train_set) + i)\n",
        "    # plt.imshow(trans(test_set[i][0]))\n",
        "    # plt.show()\n",
        "\n",
        "print(not_match_img_idx)"
      ],
      "metadata": {
        "id": "_1uFfQaNJjhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2-4. 맞지 않는 이미지 삭제"
      ],
      "metadata": {
        "id": "2ehhEF36XJe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "images = sorted(glob.glob(path_to_image+\"/*.jpg\"))\n",
        "annotations = sorted(glob.glob(path_to_annotation + \"/*.png\"))\n",
        "\n",
        "for i in not_match_img_idx:\n",
        "  print('Remove : ', images[i])\n",
        "  print('Remove : ', annotations[i])\n",
        "\n",
        "  os.remove(images[i])\n",
        "  os.remove(annotations[i])\n",
        "\n",
        "# plt.imshow(Image.open('./oxford-iiit-pet/images/staffordshire_bull_terrier_22.jpg'))\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "cIPzrZ97VP_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2-5. DataSet 및 DataLoader 재생성"
      ],
      "metadata": {
        "id": "niGe74IaXPlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터\n",
        "train_set = Pets(path_to_img=path_to_image,\n",
        "                 path_to_anno=path_to_annotation,\n",
        "                 transforms=transform)\n",
        "\n",
        "# 평가용 데이터\n",
        "test_set = Pets(path_to_img=path_to_image,\n",
        "                path_to_anno=path_to_annotation,\n",
        "                transforms=transform,\n",
        "                train=False)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_set)"
      ],
      "metadata": {
        "id": "VSoUwPzfXQG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 3. Module"
      ],
      "metadata": {
        "id": "UPv6ybM8zJYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class UNet(nn.Module):\n",
        "   def __init__(self):\n",
        "       super(UNet, self).__init__()\n",
        "\n",
        "       # ❶ U-Net의 인코더에 사용되는 은닉층\n",
        "       self.enc1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "       self.enc1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "       self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "       self.enc2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "       self.enc2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "       self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "       self.enc3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "       self.enc3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "       self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "       self.enc4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "       self.enc4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "       self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "       self.enc5_1 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
        "       self.enc5_2 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
        "\n",
        "       # 디코더에 사용되는 은닉층\n",
        "       self.upsample4 = nn.ConvTranspose2d(512, 512, 2, stride=2)\n",
        "       self.dec4_1 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
        "       self.dec4_2 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
        "\n",
        "       self.upsample3 = nn.ConvTranspose2d(256, 256, 2, stride=2)\n",
        "       self.dec3_1 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
        "       self.dec3_2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
        "\n",
        "       self.upsample2 = nn.ConvTranspose2d(128, 128, 2, stride=2)\n",
        "       self.dec2_1 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
        "       self.dec2_2 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
        "\n",
        "       self.upsample1 = nn.ConvTranspose2d(64, 64, 2, stride=2)\n",
        "       self.dec1_1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
        "       self.dec1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "       self.dec1_3 = nn.Conv2d(64, 1, kernel_size=1)\n",
        "\n",
        "       # 합성곱과 업샘플링층의 활성화 함수\n",
        "       self.relu = nn.ReLU()\n",
        "       # 출력층의 활성화함수\n",
        "       self.sigmoid = nn.Sigmoid()  # 이중분류(0과 1만 출력)이므로 출력층 시그모이드 함수 사용\n",
        "\n",
        "   def forward(self, x):\n",
        "       x = self.enc1_1(x)\n",
        "       x = self.relu(x)\n",
        "       e1 = self.enc1_2(x)  # ❶ 디코더에서 사용하기 위해 따로 변수를 지정\n",
        "       e1 = self.relu(e1)   # ❷ 합성곱층의 출력의 활성화\n",
        "       x = self.pool1(e1)\n",
        "\n",
        "       x = self.enc2_1(x)\n",
        "       x = self.relu(x)\n",
        "       e2 = self.enc2_2(x)\n",
        "       e2 = self.relu(e2)\n",
        "       x = self.pool2(e2)\n",
        "\n",
        "       x = self.enc3_1(x)\n",
        "       x = self.relu(x)\n",
        "       e3 = self.enc3_2(x)\n",
        "       e3 = self.relu(e3)\n",
        "       x = self.pool3(e3)\n",
        "\n",
        "       x = self.enc4_1(x)\n",
        "       x = self.relu(x)\n",
        "       e4 = self.enc4_2(x)\n",
        "       e4 = self.relu(e4)\n",
        "       x = self.pool4(e4)\n",
        "\n",
        "       x = self.enc5_1(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.enc5_2(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.upsample4(x)\n",
        "\n",
        "       # ❶ 인코더의 출력과 업샘플링된 이미지를 함침\n",
        "       x = torch.cat([x, e4], dim=1)\n",
        "       x = self.dec4_1(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.dec4_2(x)\n",
        "       x = self.relu(x)\n",
        "\n",
        "       x = self.upsample3(x)\n",
        "       x = torch.cat([x, e3], dim=1)\n",
        "       x = self.dec3_1(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.dec3_2(x)\n",
        "       x = self.relu(x)\n",
        "\n",
        "       x = self.upsample2(x)\n",
        "       x = torch.cat([x, e2], dim=1)\n",
        "       x = self.dec2_1(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.dec2_2(x)\n",
        "       x = self.relu(x)\n",
        "\n",
        "       x = self.upsample1(x)\n",
        "       x = torch.cat([x, e1], dim=1)\n",
        "       x = self.dec1_1(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.dec1_2(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.dec1_3(x)\n",
        "\n",
        "       x = torch.squeeze(x)  # ➋ 흑백 이미지를 그리기 위해 채널을 없앰\n",
        "\n",
        "       return x"
      ],
      "metadata": {
        "id": "QQnNmDIS5iyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 4. Learning"
      ],
      "metadata": {
        "id": "Qa9kBj02zz7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4-1. Setting"
      ],
      "metadata": {
        "id": "cg8ZCYcJ0JIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Device\n",
        "\n",
        "model = UNet().to(device) # 모델\n",
        "\n",
        "learning_rate = 0.0001  # 학습률\n",
        "\n",
        "optim = Adam(params=model.parameters(), lr=learning_rate) # 최적화"
      ],
      "metadata": {
        "id": "B8clGgz65sxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4-2. Learning"
      ],
      "metadata": {
        "id": "lH9EvJSJz-AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for epoch in range(200):\n",
        "for epoch in range(20):\n",
        "   iterator = tqdm.tqdm(train_loader)\n",
        "\n",
        "   for data, label in iterator:\n",
        "\n",
        "       optim.zero_grad()  # 이전 루프의 기울기 초기화\n",
        "\n",
        "       preds = model(data.to(device))  # 모델의 예측값 출력\n",
        "       loss = nn.BCEWithLogitsLoss()(preds, label.type(torch.FloatTensor).to(device))  # 손실 계산\n",
        "\n",
        "       loss.backward()  # 오차 역전파\n",
        "       optim.step()  # 최적화\n",
        "       iterator.set_description(f\"epoch{epoch+1} loss:{loss.item()}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"./UNet.pth\")  # 모델 가중치 저장"
      ],
      "metadata": {
        "id": "krz6kIYz52wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 5. Evaluation"
      ],
      "metadata": {
        "id": "sdk0NFRA085c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Device\n",
        "\n",
        "model.load_state_dict(torch.load(\"./UNet.pth\", map_location=\"cpu\"))\n",
        "data, label = test_set[1]\n",
        "pred = model(torch.unsqueeze(data.to(device), dim=0))>0.5  # ❶ 픽셀을 이진 분류함\n",
        "\n",
        "with torch.no_grad():\n",
        "   plt.subplot(1, 2, 1)\n",
        "   plt.title(\"Predicted\")\n",
        "   plt.imshow(pred.cpu().numpy())\n",
        "   plt.subplot(1, 2, 2)\n",
        "   plt.title(\"Real\")\n",
        "   plt.imshow(label)\n",
        "   plt.show()"
      ],
      "metadata": {
        "id": "6t5jawBx8ibI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dN2dtit8gjjG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}