{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merucode/DL/blob/01-colab-study_must_have_pytorch/10-%5Bseries-ltsm%5D-basic-ltsm-text-generater.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 0. Version check and Install Dependency"
      ],
      "metadata": {
        "id": "LxkMBWDW9T3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 0-1. Version Check"
      ],
      "metadata": {
        "id": "0Fg2WcTOqf5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "print(f\"Python version:{sys.version}\")                  # python\n",
        "print(\"Torch version:{}\".format(torch.__version__))     # torch\n",
        "print(\"cuda version: {}\".format(torch.version.cuda))    # cuda\n",
        "print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))    # cudnn"
      ],
      "metadata": {
        "id": "pvZ7Lm3Apv1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 0-2. Install Dependency"
      ],
      "metadata": {
        "id": "cNpk3WdHys1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle opencv-python\n",
        "!export export KAGGLE_USERNAME=*** && export KAGGLE_KEY=*** && kaggle datasets download -d aashita/nyt-commentsb"
      ],
      "metadata": {
        "id": "Q5g7zBYCA8Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip nyt-comments.zip"
      ],
      "metadata": {
        "id": "-yjRyPYrBHhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1. Check Data"
      ],
      "metadata": {
        "id": "TquybxQaqfE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1-1. Load data"
      ],
      "metadata": {
        "id": "OiEgwO4nzBQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import string\n",
        "\n",
        "df = pd.read_csv(\"/content/ArticlesApril2017.csv\")"
      ],
      "metadata": {
        "id": "O_1DAWvLxNST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1-2. Check data type"
      ],
      "metadata": {
        "id": "yLz34AIjzFI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "id": "BIwItmZr48ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 2. Dataset"
      ],
      "metadata": {
        "id": "dHoQK0Vw5UWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2-1. Dataset"
      ],
      "metadata": {
        "id": "CDc4vfNPBh9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "class TextGeneration(Dataset):\n",
        "    def clean_text(self, txt):\n",
        "        # 모든 단어를 소문자로 바꾸고 특수문자를 제거\n",
        "        txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "        return txt\n",
        "\n",
        "    def __init__(self):\n",
        "        all_headlines = []\n",
        "\n",
        "        # ❶ 모든 헤드라인의 텍스트를 불러옴\n",
        "        for filename in glob.glob(\"/content/*.csv\"):\n",
        "            if 'Articles' in filename:\n",
        "                article_df = pd.read_csv(filename)\n",
        "\n",
        "                # 데이터셋의 headline의 값을 all_headlines에 추가\n",
        "                all_headlines.extend(list(article_df.headline.values))\n",
        "                break\n",
        "\n",
        "        # ❷ headline 중 unknown 값은 제거\n",
        "        all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n",
        "\n",
        "        # ❸ 구두점 제거 및 전처리가 된 문장들을 리스트로 반환\n",
        "        self.corpus = [self.clean_text(x) for x in all_headlines]\n",
        "        self.BOW = {}\n",
        "\n",
        "        # ➍ 모든 문장의 단어를 추출해 고유번호 지정\n",
        "        for line in self.corpus:\n",
        "            for word in line.split():\n",
        "                if word not in self.BOW.keys():\n",
        "                    self.BOW[word] = len(self.BOW.keys())\n",
        "\n",
        "        # 모델의 입력으로 사용할 데이터\n",
        "        self.data = self.generate_sequence(self.corpus)\n",
        "\n",
        "    def generate_sequence(self, txt):\n",
        "        seq = []\n",
        "\n",
        "        for line in txt:\n",
        "            line = line.split()\n",
        "            line_bow = [self.BOW[word] for word in line]\n",
        "\n",
        "            # 단어 2개를 입력으로, 그다음 단어를 정답으로\n",
        "            data = [([line_bow[i], line_bow[i+1]], line_bow[i+2]) for i in range(len(line_bow)-2)]\n",
        "\n",
        "            seq.extend(data)\n",
        "\n",
        "        return seq\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        data = np.array(self.data[i][0])  # ❶ 입력 데이터\n",
        "        label = np.array(self.data[i][1]).astype(np.float32)  # ❷ 출력 데이터\n",
        "\n",
        "        return data, label"
      ],
      "metadata": {
        "id": "QiOBQBhaBqBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2-2. Dataloader"
      ],
      "metadata": {
        "id": "xuSV-RM8BiID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "dataset = TextGeneration()\n",
        "loader = DataLoader(dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "36iR4DvbBqN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 3. Module"
      ],
      "metadata": {
        "id": "UPv6ybM8zJYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "   def __init__(self, num_embeddings):\n",
        "       super(LSTM, self).__init__()\n",
        "\n",
        "       # ❶ 밀집표현을 위한 임베딩층\n",
        "       self.embed = nn.Embedding(\n",
        "           num_embeddings=num_embeddings, embedding_dim=16)\n",
        "\n",
        "       # LSTM을 5개층을 쌓음\n",
        "       self.lstm = nn.LSTM(\n",
        "           input_size=16,\n",
        "           hidden_size=64,\n",
        "           num_layers=5,\n",
        "           batch_first=True)\n",
        "\n",
        "       # 분류를 위한 MLP층\n",
        "       self.fc1 = nn.Linear(128, num_embeddings)\n",
        "       self.fc2 = nn.Linear(num_embeddings,num_embeddings)\n",
        "\n",
        "       # 활성화 함수\n",
        "       self.relu = nn.ReLU()\n",
        "\n",
        "   def forward(self, x):\n",
        "       x = self.embed(x)\n",
        "\n",
        "       # ❷ LSTM 모델의 예측값\n",
        "       x, _ = self.lstm(x)\n",
        "       x = torch.reshape(x, (x.shape[0], -1))\n",
        "       x = self.fc1(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.fc2(x)\n",
        "\n",
        "       return x"
      ],
      "metadata": {
        "id": "QQnNmDIS5iyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 4. Learning"
      ],
      "metadata": {
        "id": "Qa9kBj02zz7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4-1. Setting"
      ],
      "metadata": {
        "id": "cg8ZCYcJ0JIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = LSTM(num_embeddings=len(dataset.BOW)).to(device)\n",
        "optim = Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "B8clGgz65sxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4-2. Learning"
      ],
      "metadata": {
        "id": "lH9EvJSJz-AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(200):\n",
        "   iterator = tqdm.tqdm(loader)\n",
        "   for data, label in iterator:\n",
        "       # 기울기 초기화\n",
        "       optim.zero_grad()\n",
        "\n",
        "       # 모델의 예측값\n",
        "       pred = model(torch.tensor(data, dtype=torch.long).to(device))\n",
        "\n",
        "       # 정답 레이블은 long 텐서로 반환해야 함\n",
        "       loss = nn.CrossEntropyLoss()(pred, torch.tensor(label, dtype=torch.long).to(device))\n",
        "\n",
        "       # 오차 역전파\n",
        "       loss.backward()\n",
        "       optim.step()\n",
        "\n",
        "       iterator.set_description(f\"epoch{epoch} loss:{loss.item()}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"lstm.pth\")"
      ],
      "metadata": {
        "id": "krz6kIYz52wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 5. Evaluation"
      ],
      "metadata": {
        "id": "sdk0NFRA085c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, BOW, string=\"there are \", strlen=10):\n",
        "   device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "   print(f\"input word: {string}\")\n",
        "\n",
        "   with torch.no_grad():\n",
        "       for p in range(strlen):\n",
        "           # 입력 문장을 텐서로 변경\n",
        "           words = torch.tensor(\n",
        "               [BOW[w] for w in string.split()], dtype=torch.long).to(device)\n",
        "\n",
        "           # ❶\n",
        "           input_tensor = torch.unsqueeze(words[-2:], dim=0)\n",
        "           output = model(input_tensor)  # 모델을 이용해 예측\n",
        "           output_word = (torch.argmax(output).cpu().numpy())\n",
        "           string += list(BOW.keys())[output_word]  # 문장에 예측된 단어를 추가\n",
        "           string += \" \"\n",
        "\n",
        "   print(f\"predicted sentence: {string}\")\n",
        "\n",
        "model.load_state_dict(torch.load(\"lstm.pth\", map_location=device))\n",
        "pred = generate(model, dataset.BOW)"
      ],
      "metadata": {
        "id": "NB_MIf76Ez1p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}