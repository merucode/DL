{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merucode/DL/blob/01-colab-study_must_have_pytorch/04-01-%5Bimage-cnn-vgg%5D-vgg-basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxkMBWDW9T3P"
      },
      "source": [
        "## STEP 0. Version check and Install Dependency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Fg2WcTOqf5A"
      },
      "source": [
        "Step 0-1. Version Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvZ7Lm3Apv1B"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "print(f\"Python version:{sys.version}\")                  # python\n",
        "print(\"Torch version:{}\".format(torch.__version__))     # torch\n",
        "print(\"cuda version: {}\".format(torch.version.cuda))    # cuda\n",
        "print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))    # cudnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNpk3WdHys1f"
      },
      "source": [
        "Step 0-2. Install Dependency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TquybxQaqfE5"
      },
      "source": [
        "## STEP 1. Data Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiEgwO4nzBQ4"
      },
      "source": [
        "Step 1-1. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_1DAWvLxNST"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.datasets.cifar import CIFAR10\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# ❶ CIFAR10 데이터셋을 불러옴\n",
        "training_data = CIFAR10(\n",
        "    root=\"./\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor())\n",
        "\n",
        "test_data = CIFAR10(\n",
        "    root=\"./\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor())\n",
        "\n",
        "for i in range(9):\n",
        "   plt.subplot(3, 3, i+1)\n",
        "   plt.imshow(training_data.data[i])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j7LyP731D9K"
      },
      "source": [
        "## STEP 2. Dataset(with Preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLz34AIjzFI_"
      },
      "source": [
        "Step 2-1. Cropping and Flip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIwItmZr48ck"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from torchvision.datasets.cifar import CIFAR10\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomCrop\n",
        "\n",
        "transforms = Compose([ # ❶ 데이터 전처리 함수들\n",
        "   T.ToPILImage(),\n",
        "   RandomCrop((32, 32), padding=4), # ➋ 랜덤으로 이미지 일부 제거 후 패딩\n",
        "   RandomHorizontalFlip(p=0.5),     # ➌ y축으로 기준으로 대칭\n",
        "])\n",
        "\n",
        "training_data = CIFAR10(\n",
        "    root=\"./\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms) # transform에는 데이터를 변환하는 함수가 들어감\n",
        "\n",
        "test_data = CIFAR10(\n",
        "    root=\"./\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms)\n",
        "\n",
        "for i in range(9):\n",
        "   plt.subplot(3, 3, i+1)\n",
        "   plt.imshow(transforms(training_data.data[i]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlPPPXe0yz83"
      },
      "source": [
        "Step 2-2. Cropping and Flip with Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgLsRQBGztqE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from torchvision.datasets.cifar import CIFAR10\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision.transforms import RandomHorizontalFlip, RandomCrop, Normalize\n",
        "\n",
        "transforms = Compose([\n",
        "   T.ToPILImage(),\n",
        "   RandomCrop((32, 32), padding=4),\n",
        "   RandomHorizontalFlip(p=0.5),\n",
        "   T.ToTensor(),\n",
        "   # ➊ 데이터 정규화(Step 2-3 에서 구한 평균 및 표준편차 사용)\n",
        "   Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261)),\n",
        "   T.ToPILImage() # 이미지로 보기 위해 ToPIL\n",
        "])\n",
        "\n",
        "training_data = CIFAR10(\n",
        "    root=\"./\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transforms)\n",
        "test_data = CIFAR10(\n",
        "    root=\"./\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transforms)\n",
        "\n",
        "for i in range(9):\n",
        "   plt.subplot(3, 3, i+1)\n",
        "   plt.imshow(transforms(training_data.data[i]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zAhISSGz6Kp"
      },
      "source": [
        "Step 2-3. 데이터셋의 평균과 표준편차 구하기(For normalize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdYGX5bh0JWU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "training_data = CIFAR10(\n",
        "    root=\"./\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor())\n",
        "\n",
        "# item[0]은 이미지, item[1]은 정답 레이블\n",
        "imgs = [item[0] for item in training_data]\n",
        "\n",
        "# ❶imgs를 하나로 합침\n",
        "# imgs는 이미지를 여러개 담고 있는 리스트 → stack으로 합쳐서 텐서로 전환\n",
        "imgs = torch.stack(imgs, dim=0).numpy()\n",
        "\n",
        "# rgb 각각의 평균\n",
        "mean_r = imgs[:,0,:,:].mean()\n",
        "mean_g = imgs[:,1,:,:].mean()\n",
        "mean_b = imgs[:,2,:,:].mean()\n",
        "print(mean_r,mean_g,mean_b)\n",
        "\n",
        "# rgb 각각의 표준편차\n",
        "std_r = imgs[:,0,:,:].std()\n",
        "std_g = imgs[:,1,:,:].std()\n",
        "std_b = imgs[:,2,:,:].std()\n",
        "print(std_r,std_g,std_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zJNJBOu7SmG"
      },
      "source": [
        "Step 2-4. Setting Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05dJ9Npe7XKK"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "from torch.optim.adam import Adam\n",
        "\n",
        "transforms = Compose([\n",
        "   RandomCrop((32, 32), padding=4),  # ❶ 랜덤 크롭핑\n",
        "   RandomHorizontalFlip(p=0.5),  # ❷ y축으로 뒤집기\n",
        "   ToTensor(),  # ❸ 텐서로 변환\n",
        "   # ❹ 이미지 정규화\n",
        "   Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "# 학습 데이터와 평가 데이터 불러오기\n",
        "training_data = CIFAR10(root=\"./\", train=True, download=True, transform=transforms)\n",
        "test_data = CIFAR10(root=\"./\", train=False, download=True, transform=transforms)\n",
        "\n",
        "\n",
        "# 데이터로더 정의\n",
        "train_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPv6ybM8zJYk"
      },
      "source": [
        "## STEP 3. Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPl-SO7N1cuU"
      },
      "source": [
        "Step 3-1. VGG Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQnNmDIS5iyx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicBlock(nn.Module): # ❶ 기본 블록을 정의합니다.\n",
        "   # 기본블록을 구성하는 계층의 정의\n",
        "   def __init__(self, in_channels, out_channels, hidden_dim):\n",
        "       # ❷ nn.Module 클래스의 요소 상속\n",
        "       super(BasicBlock, self).__init__()\n",
        "\n",
        "       # ❸ 합성곱층 정의\n",
        "       self.conv1 = nn.Conv2d(in_channels, hidden_dim,\n",
        "                              kernel_size=3, padding=1)\n",
        "       self.conv2 = nn.Conv2d(hidden_dim, out_channels,\n",
        "                              kernel_size=3, padding=1)\n",
        "       self.relu = nn.ReLU()\n",
        "\n",
        "       # stride는 커널의 이동 거리를 의미합니다.\n",
        "       self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "   def forward(self, x): # ➍  기본블록의 순전파 정의\n",
        "       x = self.conv1(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.conv2(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.pool(x)\n",
        "\n",
        "       return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD91Gt4N1k1w"
      },
      "source": [
        "Step 3-2. CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEk2I2cV1lKr"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "   def __init__(self, num_classes): # num_classes는 클래스의 개수를 의미합니다\n",
        "       super(CNN, self).__init__()\n",
        "\n",
        "       # ❶ 합성곱 기본 블록의 정의\n",
        "       self.block1 = BasicBlock(in_channels=3, out_channels=32, hidden_dim=16)\n",
        "       self.block2 = BasicBlock(in_channels=32, out_channels=128, hidden_dim=64)\n",
        "       self.block3 = BasicBlock(in_channels=128, out_channels=256,\n",
        "                                hidden_dim=128)\n",
        "\n",
        "       # ❷ 분류기 정의\n",
        "       self.fc1 = nn.Linear(in_features=4096, out_features=2048)\n",
        "       self.fc2 = nn.Linear(in_features=2048, out_features=256)\n",
        "       self.fc3 = nn.Linear(in_features=256, out_features=num_classes)\n",
        "\n",
        "       # ❸ 분류기의 활성화 함수\n",
        "       self.relu = nn.ReLU()\n",
        "\n",
        "   def forward(self, x):\n",
        "       x = self.block1(x)\n",
        "       x = self.block2(x)\n",
        "       x = self.block3(x)  # 출력 모양: (-1, 256, 4, 4)\n",
        "       x = torch.flatten(x, start_dim=1) # ➍ 2차원 특징맵을 1차원으로\n",
        "\n",
        "       x = self.fc1(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.fc2(x)\n",
        "       x = self.relu(x)\n",
        "       x = self.fc3(x)\n",
        "\n",
        "       return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa9kBj02zz7m"
      },
      "source": [
        "## STEP 4. Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg8ZCYcJ0JIz"
      },
      "source": [
        "Step 4-1. Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8clGgz65sxJ"
      },
      "outputs": [],
      "source": [
        "from torch.optim.adam import Adam\n",
        "\n",
        "# 학습을 진행할 프로세서 설정\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# CNN 모델 정의\n",
        "model = CNN(num_classes=10)\n",
        "\n",
        "# 모델을 device로 보냄\n",
        "model.to(device)\n",
        "\n",
        "# 학습률 정의\n",
        "lr = 1e-3\n",
        "\n",
        "# 최적화 기법 정의\n",
        "optim = Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH9EvJSJz-AX"
      },
      "source": [
        "Step 4-2. Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krz6kIYz52wm"
      },
      "outputs": [],
      "source": [
        "# 학습 루프 정의\n",
        "for epoch in range(100):\n",
        "   for data, label in train_loader:  # 데이터 호출\n",
        "       optim.zero_grad()  # 기울기 초기화\n",
        "\n",
        "       preds = model(data.to(device))  # 모델의 예측\n",
        "\n",
        "       # 오차역전파와 최적화\n",
        "       loss = nn.CrossEntropyLoss()(preds, label.to(device))\n",
        "       loss.backward()\n",
        "       optim.step()\n",
        "\n",
        "   if epoch==0 or epoch%10==9:  # 10번마다 손실 출력\n",
        "       print(f\"epoch{epoch+1} loss:{loss.item()}\")\n",
        "\n",
        "\n",
        "# 모델 저장\n",
        "torch.save(model.state_dict(), \"CIFAR.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdk0NFRA085c"
      },
      "source": [
        "## STEP 5. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t5jawBx8ibI"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"CIFAR.pth\", map_location=device))\n",
        "\n",
        "num_corr = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "   for data, label in test_loader:\n",
        "\n",
        "       output = model(data.to(device))\n",
        "       preds = output.data.max(1)[1]\n",
        "       corr = preds.eq(label.to(device).data).sum().item()\n",
        "       num_corr += corr\n",
        "\n",
        "   print(f\"Accuracy:{num_corr/len(test_data)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
